# A label is worth A Thousand Images In Datset Distillation


<!-- ### | [Paper](https://arxiv.org/abs/2406.10485) -->
<br>

This repo contains the official code for Distributional Dataset Distillation with Subtask Decomposition. 

<img src='docs/D3_demo.png' width=600 img align="center">


## Basic Setup


Make sure the following pacakages are installed in your environment:   
  ```
  torch==1.13.1  
  torchvision==0.14.1  
  kornia==0.6.12  
  einops==0.6.1  
  numpy==1.20.1  
  tqdm==4.64.1
  wandb==0.13.8
  scipy==1.10.1
  ```

See `requirements.txt` for an exhaustive list of dependencies

## 1. Generating Experts
You will first need to generate experts and save intermediate checkpoints. The relevant training scripts can be found under `train_expert`

Train expert model on ImageNet: 
```bash
torchrun --nproc_per_node=1  train.py --model=resnet50 --data-path=/PATH/TO/IMATNET-1K/datasets/imagenet256 -b=256 --lr=0.0005 --output-dir=/PATH/TO-SAVE/EXPERT/CHECKPOINTS/results_100_S  --print-freq=200
```

Train expert model on TinyImageNet/CIFAR-10

```bash
python buffer.py --dataset=Tiny --model=ConvNet --train_epochs=60 --num_experts=1  --buffer_path=/PATH/TO-SAVE/EXPERT/CHECKPOINTS/results_100_S  --data_path=/PATH/TO/DATASET/data/tiny-imagenet-200 --save_interval 1
```

You can also download my pretrained expert checkpoints [here](https://drive.google.com/drive/folders/1tezdrL7YZVwBLhj8YubsuRjHrDEZ_rKA?usp=sharing)


## 2. Training Student Model with Expert-Generated Soft labels 

```bash
python nodistill.py --dataset=CIFAR100 --ipc=50 --expt_type=nothing  --teacher_label  --max_expert_epoch=104 --lr_net=1.e-02  --expert_path=/PATH/TO-YOUT/EXPERT/CHECKPOINTS/results_100_S  --data_path=/PATH/TO/DATASET/cifar100  --student_model=ConvNet --teacher_model=ConvNet --epoch_eval_train 3000 
```

Key arguments: 
* `expt_type`: experiment_type
    * nothing: default choice. Train a student network using expert labels.
    * tune_start: sweep through different expert checkpoints to identify the optimal expert epoch.
    * tune_lr: tune student model learning rate
    * other: see code
* `teacher_label`: to use soft label generated by expert (teacher)
* `max_expert_epoch`: which expert checkpoint to use (in `tune_start` mode, this arugment indicates the max expert epoch to sweep)
* `student_model`: student model architecture
* `teacher_model`: teacher model architecture
* `epoch_eval_train`: number of training epochs for the student network


## Full reproducibility
See `sample_scripts.md` for an extensive list of commands used to reprocuse all the experiment results reported in the paper. 




